{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ipynb in /Users/mattiaskallman1/anaconda3/lib/python3.7/site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipynb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"..\")\n",
    "import Models\n",
    "os.chdir(\"MockPipeline\")\n",
    "\n",
    "os.chdir(\"../../pipeline\")\n",
    "import ipynb.fs.full.processing as processing\n",
    "import ipynb.fs.full.training as training\n",
    "import ipynb.fs.full.analysis as analysis\n",
    "import ipynb.fs.full.storage as storage\n",
    "import ipynb.fs.full.visualize as visualize\n",
    "import ipynb.fs.full.misc as misc\n",
    "import ipynb.fs.full.splitting as splitting\n",
    "import ipynb.fs.full.table as table\n",
    "import ipynb.fs.full.decide as decide\n",
    "import ipynb.fs.full.real_features as features\n",
    "os.chdir(\"../models-decision-system/MockPipeline\")\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras import Input, Model\n",
    "\n",
    "from tcn import TCN, tcn_full_summary\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, mode='r') as file:\n",
    "        return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('mock_settings.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = processing.create_dataframe(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = features.add(dataframe, config['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.loc[:, dataset.columns != 'label'].to_numpy()\n",
    "labels = dataset[['label']].to_numpy()\n",
    "primary, scaler = splitting.primary(features, labels, config['splitting']['train_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_fold = splitting.timeseries(\n",
    "            primary['train'],\n",
    "            config['splitting']['validation_folds']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(linreg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = config['regression_ensemble']['models'][1]['lstm']\n",
    "lstm_fold = splitting.timeseries(\n",
    "            primary['train'],\n",
    "            config['splitting']['validation_folds'],\n",
    "            window=settings['morph']['window']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I John's kod så börjar ensemble loopen här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING\n",
    "#linreg = Models.train_model(linreg_fold[0], 'linreg', config['regression_ensemble']['models'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WORKING\n",
    "lstm = Models.train_model(lstm_fold[0], 'lstm', config['regression_ensemble']['models'][1]['lstm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattiaskallman1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(30, 4, 25)]             0         \n",
      "_________________________________________________________________\n",
      "residual_block_0 (ResidualBl [(30, 4, 64), (30, 4, 64) 13184     \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_3 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_4 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_5 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (30, 64)                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (30, 64)                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (30, 50)                  3250      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (30, 1)                   51        \n",
      "=================================================================\n",
      "Total params: 99,045\n",
      "Trainable params: 99,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# WORKING\n",
    "tcn =Models.train_model(lstm_fold[1], 'tcn', config['regression_ensemble']['models'][2]['tcn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Time Based Cross Validation](https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8)\n",
    "https://github.com/orhermansaffar/TimeBasedCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "class TimeBasedCV(object):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30\n",
    "    test_period: int\n",
    "        number of time units to include in each test set\n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is days\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='days'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='record_date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafult='record_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "                    \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        \n",
    "        start_train = validation_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        while end_test < data[date_column].max().date():\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            #print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n",
    "            #      \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            start_train = start_train + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use TimeBasedCV\n",
    "data_for_modeling=dataset.reset_index()\n",
    "tscv = TimeBasedCV(train_period=30,\n",
    "                   test_period=7,\n",
    "                   freq='days')\n",
    "for train_index, test_index in tscv.split(data_for_modeling, date_column='Date_Timestamp'):\n",
    "    continue\n",
    "\n",
    "# get number of splits\n",
    "tscv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Example- compute average test sets score: ####\n",
    "X = data_for_modeling[['Close',columns]]\n",
    "y = data_for_modeling[label]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in tscv.split(X, validation_split_date=datetime.date(2019,2,1)):\n",
    "\n",
    "    data_train   = X.loc[train_index].drop('record_date', axis=1)\n",
    "    target_train = y.loc[train_index]\n",
    "\n",
    "    data_test    = X.loc[test_index].drop('record_date', axis=1)\n",
    "    target_test  = y.loc[test_index]\n",
    "\n",
    "    # if needed, do preprocessing here\n",
    "\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # accuracy for the current fold only    \n",
    "    r2score = clf.score(data_test,target_test)\n",
    "\n",
    "    scores.append(r2score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_r2score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_base_layer_to_tcn(name, model_output, settings, index):\n",
    "    \"\"\"Support function that adds Keras Layers to TCN model.\"\"\"\n",
    "\n",
    "    # AVAILABLE LAYERS\n",
    "    available = {\n",
    "        'dropout': Dropout,\n",
    "        'dense': Dense\n",
    "    }\n",
    "\n",
    "    # SELECT THE CORRECT FUNCTION\n",
    "    func = available[name]\n",
    "\n",
    "    if name == 'dropout':\n",
    "        return func(settings['value'])(model_output)\n",
    "\n",
    "    else:\n",
    "        return func(settings['value'])(model_output)\n",
    "\n",
    "\n",
    "def add_tcn_layers(model_input, settings):\n",
    "    \"\"\"Support function that adds TCN Layer and requested Keras Layers to the TCN model\"\"\"\n",
    "\n",
    "    # STARTING LAYER (TCN)\n",
    "    tcn_string = ''\n",
    "    try:\n",
    "        for index, stats in enumerate(settings['layers'][0]['tcn']):\n",
    "            # LAYER PROPS\n",
    "            value = settings['layers'][0]['tcn'][stats]\n",
    "            if index == len(list(settings['layers'][0]['tcn']))-1:\n",
    "                tcn_string += str(stats) + '=' + str(value)\n",
    "            else:\n",
    "                tcn_string += str(stats) + '=' + str(value) + ','\n",
    "        model_output = TCN(tcn_string)(model_input)\n",
    "        print('1')\n",
    "\n",
    "    except ValueError:\n",
    "        model_output = TCN(return_sequences=False)(model_input)\n",
    "        print('2')\n",
    "    for index, layer in enumerate(settings['layers']):\n",
    "\n",
    "        # LAYER PROPS\n",
    "        name = list(layer)[0]\n",
    "        params = layer[name]\n",
    "\n",
    "        if index == 0:\n",
    "            continue\n",
    "        else:\n",
    "            model_output = add_base_layer_to_tcn(name, model_output, params, index)\n",
    "\n",
    "    return model_output\n",
    "\n",
    "\n",
    "def temporal_convolutional_network(data, settings):\n",
    "    \"\"\"Creates a Temporal Convolutional Network model (TCN) and predictions.\n",
    "\n",
    "        Args:\n",
    "            data: pandas.DataFrame.\n",
    "            settings: Dictionary object containing settings parameters.\n",
    "        Returns:\n",
    "            A dictionary containing the TCN model and predictions.\n",
    "        \"\"\"\n",
    "\n",
    "    #  TRAIN DATA GENERATOR\n",
    "    train_generator = create_generator(\n",
    "        data['train'],\n",
    "        settings['morph'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    #  TRAIN DATA GENERATOR\n",
    "    test_generator = create_generator(\n",
    "        data['test'],\n",
    "        settings['morph'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    #  INSTANTIATE KERAS TENSOR INPUT WITH TIMESERIESGENEREATOR SHAPE\n",
    "    model_input = Input(batch_shape=train_generator[0][0].shape)\n",
    "\n",
    "    #  INSTANTIATE MODEL LAYERS\n",
    "    model_output = add_tcn_layers(model_input, settings)\n",
    "\n",
    "    #  INSTANTIATE MODEL AND ASSIGN INPUT AND OUTPUT\n",
    "    model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "    # COMPILE THE MODEL\n",
    "    model.compile(optimizer=settings['optimizer'], loss=settings['loss'])\n",
    "\n",
    "    #  PRINT MODEL STATS\n",
    "    tcn_full_summary(model, expand_residual_blocks=False)\n",
    "\n",
    "    #  TRAIN THE MODEL WITH VALIDATION\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=settings['epochs'],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    #  PREDICT USING TEST DATA\n",
    "    predictions = model.predict(test_generator)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
