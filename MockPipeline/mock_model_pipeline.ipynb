{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ipynb in /Users/mattiaskallman1/anaconda3/lib/python3.7/site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipynb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"..\")\n",
    "import Models\n",
    "os.chdir(\"MockPipeline\")\n",
    "\n",
    "os.chdir(\"../../pipeline\")\n",
    "import ipynb.fs.full.processing as processing\n",
    "import ipynb.fs.full.training as training\n",
    "import ipynb.fs.full.analysis as analysis\n",
    "import ipynb.fs.full.storage as storage\n",
    "import ipynb.fs.full.visualize as visualize\n",
    "import ipynb.fs.full.misc as misc\n",
    "import ipynb.fs.full.splitting as splitting\n",
    "import ipynb.fs.full.table as table\n",
    "import ipynb.fs.full.decide as decide\n",
    "import ipynb.fs.full.real_features as features\n",
    "os.chdir(\"../models-decision-system/MockPipeline\")\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras import Input, Model\n",
    "\n",
    "from tcn import TCN, tcn_full_summary\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, mode='r') as file:\n",
    "        return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml('mock_settings.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = processing.create_dataframe(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = features.add(dataframe, config['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.loc[:, dataset.columns != 'label'].to_numpy()\n",
    "labels = dataset[['label']].to_numpy()\n",
    "primary, scaler = splitting.primary(features, labels, config['splitting']['train_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_fold = splitting.timeseries(\n",
    "            primary['train'],\n",
    "            config['splitting']['validation_folds']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = config['regression_ensemble']['models'][1]['lstm']\n",
    "lstm_fold = splitting.timeseries(\n",
    "            primary['train'],\n",
    "            config['splitting']['validation_folds'],\n",
    "            window=settings['morph']['window']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING\n",
    "linreg = Models.train_model(linreg_fold[0], 'linreg', config['regression_ensemble']['models'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattiaskallman1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/mattiaskallman1/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# WORKING\n",
    "lstm = Models.train_model(lstm_fold[1], 'lstm', config['regression_ensemble']['models'][1]['lstm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 4, 25)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(30, 4, 25)]             0         \n",
      "_________________________________________________________________\n",
      "residual_block_0 (ResidualBl [(30, 4, 64), (30, 4, 64) 13184     \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_3 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_4 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "residual_block_5 (ResidualBl [(30, 4, 64), (30, 4, 64) 16512     \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (30, 64)                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (30, 64)                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (30, 50)                  3250      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (30, 1)                   51        \n",
      "=================================================================\n",
      "Total params: 99,045\n",
      "Trainable params: 99,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# WORKING\n",
    "tcn =Models.train_model(lstm_fold[1], 'tcn', config['regression_ensemble']['models'][2]['tcn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Time Based Cross Validation](https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8)\n",
    "https://github.com/orhermansaffar/TimeBasedCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "class TimeBasedCV(object):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    train_period: int\n",
    "        number of time units to include in each train set\n",
    "        default is 30\n",
    "    test_period: int\n",
    "        number of time units to include in each test set\n",
    "        default is 7\n",
    "    freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is days\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_period=30, test_period=7, freq='days'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "\n",
    "        \n",
    "        \n",
    "    def split(self, data, validation_split_date=None, date_column='record_date', gap=0):\n",
    "        '''\n",
    "        Generate indices to split data into training and test set\n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        data: pandas DataFrame\n",
    "            your data, contain one column for the record date \n",
    "        validation_split_date: datetime.date()\n",
    "            first date to perform the splitting on.\n",
    "            if not provided will set to be the minimum date in the data after the first training set\n",
    "        date_column: string, deafult='record_date'\n",
    "            date of each record\n",
    "        gap: int, default=0\n",
    "            for cases the test set does not come right after the train set,\n",
    "            *gap* days are left between train and test sets\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        train_index ,test_index: \n",
    "            list of tuples (train index, test index) similar to sklearn model selection\n",
    "        '''\n",
    "        \n",
    "        # check that date_column exist in the data:\n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "                    \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "\n",
    "        if validation_split_date==None:\n",
    "            validation_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        \n",
    "        start_train = validation_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        while end_test < data[date_column].max().date():\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            #print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n",
    "            #      \"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            start_train = start_train + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        # mimic sklearn output  \n",
    "        index_output = [(train,test) for train,test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\n",
    "        Returns\n",
    "        -------\n",
    "        n_splits : int\n",
    "            Returns the number of splitting iterations in the cross-validator.\n",
    "        \"\"\"\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use TimeBasedCV\n",
    "data_for_modeling=dataset.reset_index()\n",
    "tscv = TimeBasedCV(train_period=30,\n",
    "                   test_period=7,\n",
    "                   freq='days')\n",
    "for train_index, test_index in tscv.split(data_for_modeling, date_column='Date_Timestamp'):\n",
    "    continue\n",
    "\n",
    "# get number of splits\n",
    "tscv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Example- compute average test sets score: ####\n",
    "X = data_for_modeling[['Close',columns]]\n",
    "y = data_for_modeling[label]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in tscv.split(X, validation_split_date=datetime.date(2019,2,1)):\n",
    "\n",
    "    data_train   = X.loc[train_index].drop('record_date', axis=1)\n",
    "    target_train = y.loc[train_index]\n",
    "\n",
    "    data_test    = X.loc[test_index].drop('record_date', axis=1)\n",
    "    target_test  = y.loc[test_index]\n",
    "\n",
    "    # if needed, do preprocessing here\n",
    "\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # accuracy for the current fold only    \n",
    "    r2score = clf.score(data_test,target_test)\n",
    "\n",
    "    scores.append(r2score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_r2score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LightGBM](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
